{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T00:34:29.735353Z",
     "start_time": "2025-11-27T00:34:27.847967Z"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURAÇÕES\n",
    "# ==============================================================================\n",
    "INPUT_DIR = Path('data/json/llm_ready')\n",
    "OUTPUT_DIR = Path('data/json/cleaned_factors')\n",
    "\n",
    "def singularizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Aplica regras básicas de singularização do português palavra por palavra.\n",
    "    Evita depender de bibliotecas externas pesadas (como spacy/nltk).\n",
    "    \"\"\"\n",
    "    palavras = texto.split()\n",
    "    palavras_singulares = []\n",
    "\n",
    "    # Lista de exceções comuns que terminam em 's' mas são singulares ou invariáveis\n",
    "    excecoes = {'ônibus', 'lápis', 'cais', 'mês', 'gás', 'país', 'simples', 'atrás', 'após', 'através', 'férias'}\n",
    "\n",
    "    for p in palavras:\n",
    "        original = p\n",
    "        # Regras de sufixo (ordem importa)\n",
    "        if p in excecoes:\n",
    "            pass\n",
    "        elif p.endswith('ns'): # jovens -> jovem, itens -> item\n",
    "            p = p[:-2] + 'm'\n",
    "        elif p.endswith('res'): # fatores -> fator, melhores -> melhor\n",
    "            p = p[:-2]\n",
    "        elif p.endswith('zes'): # vezes -> vez\n",
    "            p = p[:-2] + 'z'\n",
    "        elif p.endswith('ões'): # ações -> ação\n",
    "            p = p[:-3] + 'ão'\n",
    "        elif p.endswith('ais'): # locais -> local\n",
    "            p = p[:-3] + 'al'\n",
    "        elif p.endswith('éis') or p.endswith('eis'): # papéis -> papel, niveis -> nivel\n",
    "            p = p[:-3] + 'el'\n",
    "        elif p.endswith('s') and not p.endswith('ss'): # indeterminados -> indeterminado, humanos -> humano\n",
    "            p = p[:-1]\n",
    "\n",
    "        palavras_singulares.append(p)\n",
    "\n",
    "    return \" \".join(palavras_singulares)\n",
    "\n",
    "def padronizar_frases(texto):\n",
    "    \"\"\"\n",
    "    Realiza substituições de termos específicos (correção de preposições).\n",
    "    Deve ser rodado APÓS o texto estar em lowercase e singular.\n",
    "    \"\"\"\n",
    "    correcoes = {\n",
    "        \"aplicação de comando\": \"aplicação do comando\",\n",
    "        \"aplicação no comando\": \"aplicação do comando\",\n",
    "        \"manutenção de aeronave\": \"manutenção da aeronave\",\n",
    "        \"planejamento de voo\": \"planejamento do voo\"\n",
    "    }\n",
    "\n",
    "    for errado, correto in correcoes.items():\n",
    "        if errado in texto:\n",
    "            texto = texto.replace(errado, correto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "def limpar_fatores_contribuintes(dados_fatores):\n",
    "    \"\"\"\n",
    "    Recebe a seção 'fatores_contribuintes', limpa caracteres estranhos,\n",
    "    remove conectivos falhos, normaliza tudo para lowercase e singular.\n",
    "    \"\"\"\n",
    "    # 1. Unificação: Garante que temos uma única string para trabalhar\n",
    "    texto_bruto = \"\"\n",
    "    if isinstance(dados_fatores, list):\n",
    "        texto_bruto = \" \".join(dados_fatores)\n",
    "    elif isinstance(dados_fatores, str):\n",
    "        texto_bruto = dados_fatores\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    # 2. Normalização inicial\n",
    "    texto_bruto = re.sub(r'\\s+', ' ', texto_bruto).strip()\n",
    "\n",
    "    # 3. Estratégia de Divisão (Split por ponto e vírgula)\n",
    "    itens_brutos = re.split(r';', texto_bruto)\n",
    "\n",
    "    fatores_limpos = []\n",
    "\n",
    "    for item in itens_brutos:\n",
    "        item = item.strip()\n",
    "\n",
    "        if not item:\n",
    "            continue\n",
    "\n",
    "        # 4. Limpeza Cirúrgica (Regex)\n",
    "\n",
    "        # Passo A (ATUALIZADO): Remove conectivos \"e -\", \"e, -\", \"e. -\"\n",
    "        # Adicionamos [.,]? para aceitar pontuação após o \"e\"\n",
    "        item = re.sub(r'^\\s*(?:e[.,]?\\s+)?-?\\s*', '', item, flags=re.IGNORECASE)\n",
    "\n",
    "        # Passo B: Remove marcadores de lista comuns\n",
    "        item = re.sub(r'^[\\•\\*\\.]+\\s*', '', item)\n",
    "\n",
    "        # Passo C: Remove ponto final se estiver no fim da string\n",
    "        item = re.sub(r'\\.$', '', item)\n",
    "\n",
    "        # Passo D: Remove espaços nas pontas novamente\n",
    "        item = item.strip()\n",
    "\n",
    "        # Passo E (NOVO): Normaliza para Lowercase\n",
    "        # Transforma \"Supervisão Gerencial\" em \"supervisão gerencial\"\n",
    "        item = item.lower()\n",
    "\n",
    "        # Passo F (NOVO): Converte para Singular\n",
    "        # \"indeterminados\" -> \"indeterminado\", \"fatores humanos\" -> \"fator humano\"\n",
    "        item = singularizar_texto(item)\n",
    "\n",
    "        # Passo G (NOVO): Padronização Específica (Correções pontuais)\n",
    "        item = padronizar_frases(item)\n",
    "\n",
    "        # Só adiciona se sobrou texto útil\n",
    "        if len(item) > 2:\n",
    "            fatores_limpos.append(item)\n",
    "\n",
    "    # 5. Fallback\n",
    "    # Se a lista ficou vazia ou quase vazia, tenta estratégia agressiva (hífen)\n",
    "    if len(fatores_limpos) <= 1 and len(texto_bruto) > 50 and ';' not in texto_bruto:\n",
    "        itens_alternativos = re.split(r'\\s+-\\s+', texto_bruto)\n",
    "\n",
    "        # Aplicamos a mesma lógica de limpeza e .lower() aqui também\n",
    "        fatores_limpos = []\n",
    "        for x in itens_alternativos:\n",
    "            limpo = re.sub(r'\\.$', '', x.strip()).lower()\n",
    "            limpo = singularizar_texto(limpo) # Aplica singular aqui também\n",
    "            limpo = padronizar_frases(limpo)  # Aplica correções específicas\n",
    "            if len(limpo) > 2:\n",
    "                fatores_limpos.append(limpo)\n",
    "\n",
    "    return fatores_limpos\n",
    "\n",
    "def processar_diretorio():\n",
    "    if not INPUT_DIR.exists():\n",
    "        print(f\"Diretório não encontrado: {INPUT_DIR}\")\n",
    "        # Cria pastas dummy para teste se não existirem, apenas para não dar erro no exemplo\n",
    "        INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Criado diretório vazio para teste: {INPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    arquivos = list(INPUT_DIR.glob(\"*.json\"))\n",
    "    print(f\"Iniciando limpeza em {len(arquivos)} arquivos...\")\n",
    "\n",
    "    processados = 0\n",
    "\n",
    "    for arquivo in arquivos:\n",
    "        try:\n",
    "            with open(arquivo, 'r', encoding='utf-8') as f:\n",
    "                dados = json.load(f)\n",
    "\n",
    "            if 'conteudo' in dados and 'fatores_contribuintes' in dados['conteudo']:\n",
    "                fatores_originais = dados['conteudo']['fatores_contribuintes']\n",
    "\n",
    "                # Executa a limpeza\n",
    "                fatores_novos = limpar_fatores_contribuintes(fatores_originais)\n",
    "\n",
    "                dados['conteudo']['fatores_contribuintes'] = fatores_novos\n",
    "\n",
    "                caminho_saida = OUTPUT_DIR / arquivo.name\n",
    "                with open(caminho_saida, 'w', encoding='utf-8') as f_out:\n",
    "                    json.dump(dados, f_out, ensure_ascii=False, indent=4)\n",
    "\n",
    "                processados += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erro no arquivo {arquivo.name}: {e}\")\n",
    "\n",
    "    print(f\"Concluído! {processados} arquivos limpos salvos em '{OUTPUT_DIR}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Bloco de teste rápido para validar a lógica sem arquivos\n",
    "    print(\"--- Teste de Lógica ---\")\n",
    "    exemplo_sujo = \"Aplicação de comando; Manutenção de aeronaves; planejamento de voo.\"\n",
    "    print(f\"Entrada: {exemplo_sujo}\")\n",
    "    print(f\"Saída:   {limpar_fatores_contribuintes(exemplo_sujo)}\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "    # Executa o processamento real\n",
    "    processar_diretorio()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Teste de Lógica ---\n",
      "Entrada: Aplicação de comando; Manutenção de aeronaves; planejamento de voo.\n",
      "Saída:   ['aplicação do comando', 'manutenção da aeronave', 'planejamento do voo']\n",
      "-----------------------\n",
      "Iniciando limpeza em 1567 arquivos...\n",
      "Concluído! 1567 arquivos limpos salvos em 'data\\json\\cleaned_factors'.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3c3f2cfef6b9ca51",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
